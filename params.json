{
  "name": "Gram Cracker ",
  "tagline": "a project using image recognition with machine learning to predict Instagram Image success",
  "body": " \r\n\r\nThis was my final class project for our Intelligent Systems class.\r\nFor this project my goal was to analyze and model the growing explosive social media website Instagram.\r\n\r\n###The Question: \r\nis there an underlying model beneath the pixels of every Instagram account that could be extracted through image recognition and image modeling to successfully predict the number of likes that a user would get on a new image added to their account?\r\n\r\n###The Objective:\r\nto create an intelligent system that can look at a new user's picture and predict the number of likes it would receive.\r\n\r\n### The Approach:\r\n\r\nMy initial approach for this project was to use Artificial Neural Networks to analyze images and see if an underlying model existed on the appearance of an image within a user account to successfully determine the success that a given image would have.\r\n\r\nNeural Network Structure:\r\n![ANN](https://github.com/jfelzone/cs5660AI/blob/master/images/neuralNet.jpeg?raw=true)\r\n\r\nThe first thing needed was to gather the data. To gather the data I implemented three main libraries and techniques. I used pyautogui, pytesseract, and PIL to gather my images from Instagram.\r\n\r\n```python\r\nimport pyautogui\r\nimport pytesseract\r\nfrom PIL import Image\r\n```\r\n\r\n### Gathering the Data:\r\nBefore using the three libraries above I actually attempted to use the Instagram API, but quickly read and discovered that the number of API calls I could make an hour simply wasn't going to be enough for the amount of data that I wished to collect. Thus my reasoning for using the above libraries.\r\n\r\nPyautogui is a python library that allows for the manipulation and usage of the gui features of your computer (such as your mouse and keyboard). I used this library (along with PIL imaging library) to manipulate the browser on my computer screen to scroll through instagram accounts, and take screenshots of each picture within the account. This was how I gathered my images.\r\n\r\nI then used pytesseract to extract the number of likes from the screenshots that I took. The library is a wrapper to Google's OCR and allows for text extraction from images. With enlarged and zoomed screenshots I gained very high accuracy in being able to extract likes from an image. Essentially I was detecting the below piece highlighted in red:\r\n\r\n#####pytesseract like extraction:\r\n![Like Extract](https://github.com/jfelzone/cs5660AI/blob/master/images/likeExtract.png?raw=true)\r\n\r\nA video of the file **gatherInsta.py** in action can be seen below: (this shows the gui manip in action, click image to follow link to YouTube)\r\n\r\n[![Video](https://github.com/jfelzone/cs5660AI/blob/master/images/imageGather.png?raw=true)](https://www.youtube.com/watch?v=hiEfYRPM93M&t=62s)\r\n\r\n### Pre-Processing Observations, Account Behavor Analysis:\r\n\r\nNow, with the data in my hand, I was ready to jump into the algorithm and prediciton analysis of the data. However, before doing this, I did an inital analysis on the behavior of various accounts over time. This is shown in the **accountPlotsOverTime.py** file.\r\nwithin this file I used the following libraries:\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport time\r\n```\r\nUsing matplot lib I was able to generate several scatter plot graphs as well as numpy to calculate the behavor of an account over time. What was observed is that typically speaking an account that has gained traction will have the following scatter plot representation:\r\n\r\n![Neg Res](https://github.com/jfelzone/cs5660AI/blob/master/images/plotOverTimeNeg.png?raw=true \"negative regression\")\r\n\r\nWe can observe from this image that typically as an account gains traction the increase in likes over time generally increases. As the x axis increases on the graph we are seeing a further and further look back in time for the given Instagram account.\r\nAbout 70% of the accounts that I pulled data for had the following format with a negative slope on the regression line.\r\nHowever there were a few with the following format as well:\r\n![Positive Regression](https://github.com/jfelzone/cs5660AI/blob/master/images/plotOverTimePos.png?raw=true \"positive regression\")\r\n\r\nAs we can observe, there is very little correlation amongst the images as well as a positive regression line. This would lead us to assume that there will be inherent error within our results as we do not have images that seem to follow and overly distinct pattern. Thus, initially I hypothesized that our margin of error could be quite large on images that do not follow a consistent like count over time.\r\n\r\n### The Meat of It: Algorithm Time:\r\n\r\nThe first neural network library that I used was a python Neural Network library called PyBrain. The usage for this library can be seen in the following file **neuralNetwork.py**\r\nTo import pybrain the following import statements were use:\r\n\r\n```python\r\nimport pyautogui\r\nfrom pybrain.datasets.supervised import SupervisedDataSet \r\nfrom pybrain.tools.shortcuts import buildNetwork\r\nfrom pybrain.supervised.trainers import BackpropTrainer\r\nimport cv2\r\nimport time\r\nimport math\r\n```\r\n\r\nThese imports allowed me to build a neural network within pybrain. I used cv2 to read in images and then convert them into one dimensional arrays of RGB pixel values.\r\n\r\nWe can see the code in the file starting on line 73 which added samples to the neural network based on the loadImage function that generated the list/array of pixel values:\r\n\r\n```python\r\nfor i in range(0,21):\r\n\t\ttry:\r\n\t\t\tds.addSample(loadImage(imgdirectory+'img_'+str(i)+'.png'), (imgHash['img_'+str(i)+'.png'],))\r\n\t\texcept:\r\n\t\t\tprint \"Image does not exist, cannot be added to training data\"\r\n\r\n\ttrainer = BackpropTrainer(net, ds)\r\n\terror = 10\r\n\titeration = 0\r\n\twhile error > 0.001:\r\n\t\terror = trainer.train()\r\n\t\titeration += 1\r\n\t\tprint \"Iteration: {0} Error {1}\".format(iteration, error)\r\n```\r\n\r\nThis library was moderately successful when tested. I had an average margin of error of 30.3% prediction. This metric was derived from scientific error based on the comparison of the actual result and the experimental result and then calculating the error between them. This was a fairly small margin however I was only able to train 20 images in 6.7 hours.\r\nI needed to find something better.\r\n\r\n###The Sci-Kit Learn Stack:\r\n\r\nAt this point in my project I decided I would try to use Sci-Kit learn to test the speed performance in comparison to PyBrain. PyBrain was advertised to be extremely fast and scalable, thus I assumed it would be one of the better libraries, and Sci-Kit learn says on its documentation page that it is not industry speed standard. However, after reading that Sci-Kit learn is built on top of numpy and scipy I figured there was potential for not only a higher performance in speed, but also an increased performance in accuracy. \r\n\r\nThus, I created the following first test file using Sci-Kit Learn: **sciKitNeuralNetwork.py**\r\nI needed to import the following lines of code to run the algorithm:\r\n```python\r\nfrom sklearn.neural_network import MLPClassifier\r\nimport time\r\nimport cv2\r\nimport random\r\n```\r\n\r\nIn order to call the Neural Network classifier (MLP; Multi-Layered Perceptron) I ran the following code on lines 78 and 79:\r\n\r\n```python\r\nclf = MLPClassifier(activation='logistic', solver='lbfgs', alpha=0.001, hidden_layer_sizes=(35, 20), random_state=1, max_iter=1000)\r\n\tclf.fit(X, y)\r\n```\r\n\r\nI loaded the images the same way as I did using PyBrain and found a drastic speed boost using Sci-Kit learn. I could load 4 times as many images in a sixth of the time.\r\n\r\nWith this increased speed boost, and ease of access I then decided to test several different algorithms. \r\nThese are found in the following files: **svmInsta.py** **randomForestInsta.py** where I tested my results using both SVM and Random Forest Algorithms.\r\nThe results are shown below:\r\n\r\n![results](https://github.com/jfelzone/cs5660AI/blob/master/images/results.png?raw=true)\r\n\r\nThe second metric was more of a curiosity thing on my part. I wanted to know the percentage of the time that the algorithm predicted within the correct number of digits as well. We would expect this to be much higher and it indeed was. \r\n\r\n### My final steps\r\n\r\nAt this point, I had gotten some very interesting results. I would never have expected a Random Forest algorithm to handle image recognition better than a neural network, however, that was the case. My lowest scientific margin of error was found using a random forest. Where on average my model was predicting off by only 38%.\r\n\r\nAt this point, I dove down one more rabbit hole. I wanted to be able to load more images in order to increase my training set and thus increase my accuracy. To do this I used the **imageReduction.py** script. \r\n\r\nThis file had one main difference. It shrunk the features of the images by converting them to a grey scale image. This code is show below:\r\n\r\n```python\r\ndef smallImageLoad(path):\r\n\timage = Image.open(path).convert('L')\r\n\tpix = np.asarray(image)\r\n\toneDpixelArray = []\r\n\tfor i in pix:\r\n\t\tfor j in i:\r\n\t\t\toneDpixelArray.append(j)\r\n\r\n\treturn oneDpixelArray\r\n```\r\n\r\nThis code thus shrunk my 1D arrays from 270,000 elements to only 90,000 and I was able to load many more images into memory in order to run my final results.\r\n\r\nWith this improved data set, I found an increase in my predictions.\r\n\r\n```python\r\nprint \"Training the classifier\"\r\n\t\t# i want to see how slow NN is now\r\n\t\t#clf = MLPClassifier(activation='logistic', solver='lbfgs', alpha=0.001, hidden_layer_sizes=(35, 20), random_state=1, max_iter=1000)\r\n\t\tclf = RandomForestClassifier(n_estimators=500)\r\n\t\tclf.fit(X, y)\r\n```\r\n\r\nAfter running both the Neural Network and Random Forests I got the following improved results:\r\n\r\n#### Neural Network Margin of Error: 20%\r\n#### Random Forest Margin of Error 22%\r\n\r\n### In Conclusion\r\n\r\nAs we can conclude, I got some interesting results from this project as well as developed a baseline framework for continuing to pursue an algorithm that could predict Instagram image success within a margin of error of as low as 5% or 10%. That will be my goal in the continuation of this project after the course of the class. I intend to look into other features that could be added to training set in order to get an even higher prediction accuracy.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}