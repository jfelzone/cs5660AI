{
  "name": "Cs5660ai",
  "tagline": "a repository/website to publish my cs5660 AI final project",
  "body": "# Welcome to the cs5660_AI Instagram Github Wiki!\r\n\r\nThis was my final class project for our Intelligent Systems class.\r\nFor this project my goal was to analyze and model the growing explosive social media website Instagram.\r\n\r\n###The Question: \r\nis there an underlying model beneath the pixels of every Instagram account that could be extracted through image recognition and image modeling to successfully predict the number of likes that a user would get on a new image added to their account?\r\n\r\n###The Objective:\r\nto create an intelligent system that can look at a new user's picture and predict the number of likes it would receive.\r\n\r\n### The Approach:\r\n\r\nMy initial approach for this project was to use Artificial Neural Networks to analyze images and see if an underlying model existed on the appearance of an image within a user account to successfully determine the success that a given image would have.\r\n\r\nNeural Network Structure:\r\n![alt text](cs5660AI/images/neuralNet.jpeg \"Neural Network\")\r\n![ANN](https://github.com/jfelzone/cs5660AI/blob/master/images/neuralNet.jpeg)\r\n\r\nThe first thing needed was to gather the data. To gather the data I implemented two main libraries and techniques. I used pyautogui, pytesseract, and PIL to gather my images from Instagram.\r\n\r\n```python\r\nimport pyautogui\r\nimport pytesseract\r\nfrom PIL import Image\r\n```\r\n\r\n### Gathering the Data:\r\nBefore using the three libraries above I actually attempted to use the Instagram API, but quickly read and discovered that the number of API calls I could make an hour simply wasn't going to be enough for the amount of data that I wished to collect. Thus my reasoning for using the above libraries.\r\n\r\nPyautogui is a python library that allows for the manipulation and usage of the gui features of your computer (such as your mouse and keyboard). I used this library (along with PIL imaging library) to manipulate the browser on my computer screen to scroll through instagram accounts, and take screenshots of each picture within the account. This was how I gathered my images.\r\n\r\nI then used pytesseract to extract the number of likes from the screenshots that I took. The library is a wrapper to Google's OCR and allows for text extraction from images. With enlarged and zoomed screenshots I gained very high accuracy in being able to extract likes from an image. Essentially I was detecting the below piece highlighted in red:\r\n\r\n#####pytesseract like extraction:\r\n![alt text](https://github.com/jfelzone/cs5660_AI/blob/master/images/likeExtract.png \"like extract\")\r\n\r\nA video of the file **gatherInsta.py** in action can be seen below: (this shows the gui manip in action, click image to follow link to YouTube)\r\n\r\n[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/hiEfYRPM93M/0.jpg)](http://www.youtube.com/watch?v=hiEfYRPM93M)\r\n\r\n### Pre-Processing Observations, Account Behavor Analysis:\r\n\r\nNow, with the data in my hand, I was ready to jump into the algorithm and prediciton analysis of the data. However, before doing this, I did an inital analysis on the behavior of various accounts over time. This is shown in the **accountPlotsOverTime.py** file.\r\nwithin this file I used the following libraries:\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport time\r\n```\r\nUsing matplot lib I was able to generate several scatter plot graphs as well as numpy to calculate the behavor of an account over time. What was observed is that typically speaking an account that has gained traction will have the following scatter plot representation:\r\n\r\n![alt text](https://github.com/jfelzone/cs5660_AI/blob/master/images/plotOverTimeNeg.png \"negative regression\")\r\n\r\nWe can observe from this image that typically as an account gains traction the increase in likes over time generally increases. As the x axis increases on the graph we are seeing a further and further look back in time for the given Instagram account.\r\nAbout 70% of the accounts that I pulled data for had the following format with a negative slope on the regression line.\r\nHowever there were a few with the following format as well:\r\n![alt text](https://github.com/jfelzone/cs5660_AI/blob/master/images/plotOverTimePos.png \"positive regression\")\r\n\r\nAs we can observe, there is very little correlation amongst the images as well as a positive regression line. This would lead us to assume that there will be inherant errer within our results as we do not have images that seem to follow and overly distinct pattern. Thus, initally I hypothesized that our margin of error could be quite large on images that do not follow a consisten like count over time.\r\n\r\n### The Meat of It: Algorithm Time:\r\n\r\nThe first neural network library that I used was a python Neural Network library called PyBrain. The usage for this library can be seen in the following file **neuralNetwork.py**\r\nTo import pybrain the following import statements were use:\r\n\r\n```python\r\nimport pyautogui\r\nfrom pybrain.datasets.supervised import SupervisedDataSet \r\nfrom pybrain.tools.shortcuts import buildNetwork\r\nfrom pybrain.supervised.trainers import BackpropTrainer\r\nimport cv2\r\nimport time\r\nimport math\r\n```\r\n\r\nThese imports allowed me to build a neural network within pybrain. I used cv2 to read in images and then convert them into one dimensional arrays of RGB pixel values.\r\n\r\nWe can see the code in the file starting on line 73 which added samples to the neural network based on the loadImage function that generated the list/array of pixel values:\r\n\r\n```python\r\nfor i in range(0,21):\r\n\t\ttry:\r\n\t\t\tds.addSample(loadImage(imgdirectory+'img_'+str(i)+'.png'), (imgHash['img_'+str(i)+'.png'],))\r\n\t\texcept:\r\n\t\t\tprint \"Image does not exist, cannot be added to training data\"\r\n\r\n\ttrainer = BackpropTrainer(net, ds)\r\n\terror = 10\r\n\titeration = 0\r\n\twhile error > 0.001:\r\n\t\terror = trainer.train()\r\n\t\titeration += 1\r\n\t\tprint \"Iteration: {0} Error {1}\".format(iteration, error)\r\n```\r\n\r\nThis library was moderately successful when tested. I had an average margin of error of 30.3% prediction. This metric was derived from scientific error based on the comparison of the actual result and the experimental result and then calculating the error between them. This was a fairly small margin however I was only able to train 20 images in 6.7 hours.\r\nI needed to find something better.\r\n\r\n###The Sci-Kit Learn Stack:\r\n\r\nAt this point in my project I decided I would try to use Sci-Kit learn to test the speed performance in comparison to PyBrain. PyBrain was advertised to be extremely fast and scalable, thus I assumed it would be one of the better libraries, and Sci-Kit learn says on its documentation page that it is not industry speed standard. However, after reading that Sci-Kit learn is built on top of numpy and scipy I figured there was potential for not only a higher performance in speed, but also an increased performance in accuracy. \r\n\r\nThus, I created the following first test file using Sci-Kit Learn: **sciKitNeuralNetwork.py**\r\nI needed to import the following lines of code to run the algorithm:\r\n```python\r\nfrom sklearn.neural_network import MLPClassifier\r\nimport time\r\nimport cv2\r\nimport random\r\n```\r\n\r\nIn order to call the Neural Network classifier (MLP; Multi-Layered Perceptron) I ran the following code on lines 78 and 79:\r\n\r\n```python\r\nclf = MLPClassifier(activation='logistic', solver='lbfgs', alpha=0.001, hidden_layer_sizes=(35, 20), random_state=1, max_iter=1000)\r\n\tclf.fit(X, y)\r\n```\r\n\r\nI loaded the images the same way as I did using PyBrain and found a drastic speed boost using Sci-Kit learn. I could load 4 times as many images in a sixth of the time.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}